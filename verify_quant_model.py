import tensorflow as tf
import numpy as np
import os
from tqdm import tqdm # ç”¨äºæ˜¾ç¤ºè¿›åº¦æ¡
import re
import time

from mobile_cv.model_zoo.models.fbnet_tf_a import FBNetAKeras
from mobile_cv.model_zoo.models.fbnet_v2 import fbnet

GROUND_TRUTH_PATH = '/root/autodl-tmp/ILSVRC2012_devkit_t12/data/ILSVRC2012_validation_ground_truth.txt'
# --- é…ç½®å‚æ•° ---
TFLITE_MODEL_PATH = '/root/autodl-tmp/deploy/fbnet_a_imagenet_int8_224x224_model.tflite' # æ›¿æ¢ä¸ºä½ çš„é‡åŒ–æ¨¡å‹è·¯å¾„
NUM_TEST_SAMPLES = 1000  # ç”¨äºç²¾åº¦è¯„ä¼°çš„æ ·æœ¬æ•°é‡
INPUT_SIZE = 224
IMAGENET_VAL_DIR = '/root/autodl-tmp/imagenet-1k' # æ›¿æ¢ä¸ºä½ çš„ ImageNet éªŒè¯é›†ç›®å½•

keras_model_path = '/root/autodl-tmp/deploy/fbnet_a.keras'
keras_model = tf.keras.models.load_model(
    keras_model_path,
    custom_objects={'FBNetAKeras': FBNetAKeras} # å°†ç±»åæ˜ å°„åˆ°å®é™…çš„ Python ç±»
)
input_shape_without_batch = (224, 224, 3) 
keras_model.build(input_shape=(None,) + input_shape_without_batch)

# PyTorch Normalize å‚æ•°
IMAGENET_MEAN = tf.constant([0.485, 0.456, 0.406])
IMAGENET_STD = tf.constant([0.229, 0.224, 0.225])
INPUT_CROP_SIZE = 224

def load_imagenet_ground_truth(file_path):
    """ä»æ–‡ä»¶ä¸­åŠ è½½ ImageNet éªŒè¯é›†çš„æ‰€æœ‰çœŸå®æ ‡ç­¾ã€‚"""
    print(f"æ­£åœ¨åŠ è½½çœŸå®æ ‡ç­¾æ–‡ä»¶: {file_path}")
    with open(file_path, 'r') as f:
        # æ ‡ç­¾æ–‡ä»¶æ˜¯ 1-based ç´¢å¼•
        # æˆ‘ä»¬å°†å®ƒå­˜å‚¨åœ¨ä¸€ä¸ªåˆ—è¡¨ä¸­ï¼Œç´¢å¼• 0 å¯¹åº”ç¬¬ä¸€å¼ å›¾ (0-based)
        labels = [int(line.strip()) for line in f if line.strip()]
    return labels


def tf_fbnet_preprocess(image_path):
    """
    TensorFlow å®ç° FBNet å®˜æ–¹ä»“åº“çš„ PyTorch é¢„å¤„ç†é€»è¾‘ã€‚
    å‡è®¾ç›®æ ‡ resize å°ºå¯¸æ˜¯ 256ã€‚
    """
    
    # --- 1. è¯»å–å›¾åƒ ---
    img = tf.io.read_file(image_path)
    img = tf.image.decode_jpeg(img, channels=3) # åŸå§‹ uint8 [0, 255]

    # --- 2. ç¼©æ”¾æœ€çŸ­è¾¹åˆ° 256 ---
    # TensorFlow æ¨èä½¿ç”¨ tf.image.resize å’Œ tf.image.resize_with_pad å®ç°ï¼Œ
    # ä½†æœ€ç²¾ç¡®çš„æ–¹æ³•æ˜¯æ‰‹åŠ¨è®¡ç®—å®½é«˜æ¯”ã€‚
    original_shape = tf.shape(img)
    orig_h = tf.cast(original_shape[0], tf.float32)
    orig_w = tf.cast(original_shape[1], tf.float32)
    
    # ç¡®å®šç¼©æ”¾å› å­ï¼šç¼©æ”¾æœ€çŸ­è¾¹åˆ° 256
    scale_factor = tf.cond(orig_h < orig_w, 
                           lambda: 256.0 / orig_h, 
                           lambda: 256.0 / orig_w)
    
    new_h = tf.cast(orig_h * scale_factor, tf.int32)
    new_w = tf.cast(orig_w * scale_factor, tf.int32)
    
    # æ‰§è¡Œç¼©æ”¾
    resized_img = tf.image.resize(img, [new_h, new_w], method=tf.image.ResizeMethod.BILINEAR)

    # --- 3. ä¸­å¿ƒè£å‰ªåˆ° 224x224 ---
    cropped_img = tf.image.central_crop(resized_img, central_fraction=INPUT_CROP_SIZE / tf.cast(tf.minimum(new_h, new_w), tf.float32))

    # ç”±äº tf.image.central_crop æ¥å— fractionï¼Œä½¿ç”¨ tf.image.crop_to_bounding_box æ›´ç²¾ç¡®æ§åˆ¶å°ºå¯¸
    # é‡æ–°è®¡ç®—è£å‰ªèµ·å§‹ç‚¹
    h_start = (new_h - INPUT_CROP_SIZE) // 2
    w_start = (new_w - INPUT_CROP_SIZE) // 2
    cropped_img = tf.image.crop_to_bounding_box(resized_img, h_start, w_start, INPUT_CROP_SIZE, INPUT_CROP_SIZE)

    # --- 4. è½¬æ¢ä¸º [0, 1] çš„ Float32 ---
    # PyTorch ToTensor() è¡Œä¸º
    normalized_img = tf.image.convert_image_dtype(cropped_img, tf.float32) 

    # --- 5. æ ‡å‡†åŒ– (Normalize) ---
    # PyTorch: CHW format -> TF: HWC format, éœ€è¦æ‰©å±•ç»´åº¦å¹¶é‡æ–°æ’åºï¼Œä½† tf.image å·²ç»åœ¨ HWC ä¸Šæ“ä½œ
    # mean å’Œ std éœ€è¦æ‰©å±•åˆ° HWC çš„å½¢çŠ¶è¿›è¡Œå¹¿æ’­
    mean_tensor = tf.reshape(IMAGENET_MEAN, [1, 1, 3])
    std_tensor = tf.reshape(IMAGENET_STD, [1, 1, 3])
    
    # img = (img - mean) / std
    final_img = (normalized_img - mean_tensor) / std_tensor
    
    # è¿”å› HWC æ ¼å¼çš„æœ€ç»ˆå¼ é‡
    return final_img

def load_test_data(num_samples):
    """
    åŠ è½½ ImageNet æµ‹è¯•å›¾åƒåŠå…¶çœŸå®æ ‡ç­¾ã€‚
    è¦æ±‚å›¾åƒæ–‡ä»¶åä¸º ILSVRC2012_val_00000001.JPEG æ ¼å¼ã€‚
    """
    # 1. åŠ è½½æ‰€æœ‰çœŸå®æ ‡ç­¾
    try:
        all_labels_1_based = load_imagenet_ground_truth(GROUND_TRUTH_PATH)
    except FileNotFoundError:
        print(f"é”™è¯¯: æ ‡ç­¾æ–‡ä»¶æœªæ‰¾åˆ°åœ¨ {GROUND_TRUTH_PATH}ã€‚æ— æ³•è¿›è¡Œç²¾åº¦è¯„ä¼°ã€‚")
        return []

    image_files = os.listdir(IMAGENET_VAL_DIR)
    
    # è¿‡æ»¤æ‰é JPEG æ–‡ä»¶
    image_files = [f for f in image_files if f.endswith(('.JPEG', '.jpg', '.jpeg'))]

    # éšæœºé€‰æ‹©æ ·æœ¬
    # np.random.shuffle(image_files) # å¦‚æœéœ€è¦éšæœºæ€§ï¼Œè¯·è§£é™¤æ³¨é‡Š
    selected_files = image_files[:num_samples]
    
    test_data = [] 
    
    print(f"æ­£åœ¨åŠ è½½å¹¶é¢„å¤„ç† {len(selected_files)} ä¸ªå›¾åƒæ ·æœ¬...")

    # æ­£åˆ™è¡¨è¾¾å¼ç”¨äºä»æ–‡ä»¶åä¸­æå–åºå·
    # ILSVRC2012_val_00000001.JPEG -> 1
    file_pattern = re.compile(r'ILSVRC2012_val_(\d+)\.JPEG', re.IGNORECASE)

    for filename in selected_files:
        match = file_pattern.search(filename)
        
        if match:
            # æå–åºå· (ä¾‹å¦‚ '00000001' -> 1)
            file_index_1_based = int(match.group(1)) 
            # æ ‡ç­¾åˆ—è¡¨æ˜¯ 0-basedï¼Œæ‰€ä»¥ç´¢å¼•éœ€è¦å‡ 1
            label_index_0_based = file_index_1_based - 1
            
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦æœ‰æ•ˆ
            if 0 <= label_index_0_based < len(all_labels_1_based):
                true_label = all_labels_1_based[label_index_0_based]
                
                image_path = os.path.join(IMAGENET_VAL_DIR, filename)
                
                # é¢„å¤„ç†
                processed_image = tf_fbnet_preprocess(image_path)
                processed_image = tf.expand_dims(processed_image, axis=0) 
                
                # å­˜å‚¨ [è¾“å…¥å¼ é‡, çœŸå®æ ‡ç­¾]
                test_data.append((processed_image.numpy(), true_label))
            else:
                print(f"è­¦å‘Š: æ–‡ä»¶ {filename} çš„ç´¢å¼• {label_index_0_based} è¶…å‡ºæ ‡ç­¾èŒƒå›´ã€‚è·³è¿‡ã€‚")
        else:
            print(f"è­¦å‘Š: æ–‡ä»¶å {filename} ä¸ç¬¦åˆæ ‡å‡† ImageNet éªŒè¯é›†æ ¼å¼ã€‚è·³è¿‡ã€‚")
    print(f'æ•°æ®é›†é•¿åº¦{len(test_data)}')
    return test_data
# =================================================================
# 2. TFLite Interpreter æ¨ç†å‡½æ•°
# =================================================================

def run_tflite_inference(tflite_model_path, test_data):
    # åˆå§‹åŒ– TFLite Interpreter
    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)
    interpreter.allocate_tensors()

    # è·å–è¾“å…¥å’Œè¾“å‡ºå¼ é‡çš„ç´¢å¼•
    input_details = interpreter.get_input_details()[0]
    output_details = interpreter.get_output_details()[0]

    # TFLite æ¨ç†å¾ªç¯
    tflite_outputs = []
    
    # è·å– TFLite æ¨¡å‹çš„æœŸæœ›è¾“å…¥ç±»å‹
    # å¦‚æœå…¨æ•´æ•°é‡åŒ–æˆåŠŸï¼Œè¿™é‡Œåº”è¯¥æ˜¯ tf.int8 æˆ– tf.uint8
    input_dtype = input_details['dtype']
    
    print(f"\nğŸ“¢ TFLite æ¨¡å‹æœŸæœ›è¾“å…¥ç±»å‹: {input_dtype}")

    start_time = time.time()

    for input_tensor, label in tqdm(test_data, desc="TFLite æ¨ç†ä¸­"):
        # 1. é‡åŒ–è¾“å…¥ï¼ˆå¦‚æœ TFLite æ¨¡å‹è¦æ±‚ INT8 è¾“å…¥ï¼‰
        if input_dtype == np.int8 or input_dtype == np.uint8:
            # è·å–é‡åŒ–å‚æ•°
            scale, zero_point = input_details['quantization']
            try:
                info = np.iinfo(input_dtype)
                input_min_value = info.min
                input_max_value = info.max
            except ValueError:
                # å¦‚æœæ˜¯ float32 æˆ–å…¶ä»–éæ•´æ•°ç±»å‹ï¼Œåˆ™ç›´æ¥ä½¿ç”¨ np.finfo
                info = np.finfo(input_dtype)
                input_min_value = info.min
                input_max_value = info.max
            # å°† Float32 è¾“å…¥é‡åŒ–ä¸º INT8
            if scale == 0:
                quantized_input = input_tensor.astype(input_dtype)
            else:
                quantized_input = input_tensor / scale + zero_point
                quantized_input = np.clip(quantized_input, input_min_value, input_max_value).astype(input_dtype)
        else:
            # å¦‚æœ TFLite ä»ç„¶è¦æ±‚ Float32 è¾“å…¥ (åŠ¨æ€èŒƒå›´é‡åŒ–)
            quantized_input = input_tensor

        
        # 2. è®¾ç½®è¾“å…¥
        interpreter.set_tensor(input_details['index'], quantized_input)
        
        # 3. æ‰§è¡Œæ¨ç†
        interpreter.invoke()
        
        # 4. è·å–è¾“å‡º
        tflite_output = interpreter.get_tensor(output_details['index'])
        
        # 5. åé‡åŒ–è¾“å‡ºï¼ˆå¦‚æœè¾“å‡ºæ˜¯ INT8ï¼‰
        if output_details['dtype'] == np.int8 or output_details['dtype'] == np.uint8:
            output_scale, output_zero_point = output_details['quantization']
            if output_scale != 0:
                tflite_output = (tflite_output.astype(np.float32) - output_zero_point) * output_scale
        
        tflite_outputs.append(tflite_output)

    end_time = time.time()
    
    total_time = end_time - start_time
    fps = len(test_data) / total_time
    
    print(f"TFLite æ¨ç†å®Œæˆã€‚æ€»è€—æ—¶: {total_time:.4f} ç§’")
    print(f"TFLite ååé‡ (FPS): {fps:.2f} æ ·æœ¬/ç§’")
        
    return np.array(tflite_outputs)

# =================================================================
# 3. Keras æ¨¡å‹æ¨ç†å‡½æ•° (ä½œä¸º Float32 åŸºå‡†)
# =================================================================

def run_keras_inference(keras_model, test_data):
    keras_inputs = [data[0] for data in test_data]
    
    # Keras æ¨ç†é€šå¸¸æ¯” TFLite Interpreter å¿«
    print("\nğŸš€ Keras (Float32) æ¨ç†ä¸­...")

    # keras_outputs = []
    # total_inference_time = 0
    # for input_tensor, label in tqdm(test_data, desc="Keras å•æ ·æœ¬æ¨ç†ä¸­"):
    #     start_time = time.time()
    #     # å¼ºåˆ¶ batch_size=1
    #     output = keras_model.predict(input_tensor, batch_size=1, verbose=0) 
    #     end_time = time.time()
    #     total_inference_time += (end_time - start_time)
    #     keras_outputs.append(output)

    # keras_outputs = np.array(keras_outputs)

    # æ³¨æ„ï¼šè¿™é‡Œçš„ keras_model åº”è¯¥è¢«ç¦ç”¨ GPU ä»¥ä¾¿å…¬å¹³æ¯”è¾ƒï¼Œä½†ä¸ºäº†ç®€å•ï¼Œç›´æ¥è¿è¡Œ
    start_time = time.time()
    keras_outputs = keras_model.predict(np.vstack(keras_inputs), batch_size=32, verbose=0)
    end_time = time.time()
    
    total_time = end_time - start_time
    fps = len(test_data) / total_time
    
    print(f"Keras æ¨ç†å®Œæˆã€‚æ€»è€—æ—¶: {total_time:.4f} ç§’")
    print(f"Keras ååé‡ (FPS): {fps:.2f} æ ·æœ¬/ç§’")
    
    return keras_outputs

# =================================================================
# 4. ç²¾åº¦è¯„ä¼°ä¸æŠ¥å‘Š
# =================================================================

def evaluate_and_report(tflite_outputs, keras_outputs, test_data):
    # è·å–çœŸå®æ ‡ç­¾
    true_labels = np.array([data[1] for data in test_data])
    
    # é¢„æµ‹ç±»åˆ«
    tflite_preds = np.argmax(tflite_outputs, axis=1)
    keras_preds = np.argmax(keras_outputs, axis=1)
    
    # 1. è®¡ç®— Top-1 ç²¾åº¦ (éœ€è¦çœŸå®æ ‡ç­¾)
    tflite_accuracy = np.mean(tflite_preds == true_labels)
    keras_accuracy = np.mean(keras_preds == true_labels)

    # 2. è®¡ç®—è¾“å‡ºå·®å¼‚ (é‡åŒ–æŸå¤±çš„ç›´æ¥æŒ‡æ ‡ï¼Œä¸éœ€è¦çœŸå®æ ‡ç­¾)
    # ä½¿ç”¨ L2 èŒƒæ•°æˆ–ä½™å¼¦ç›¸ä¼¼åº¦æ¯”è¾ƒ Float32 å’Œ INT8 çš„è¾“å‡º logits
    output_difference = np.linalg.norm(tflite_outputs - keras_outputs, axis=1).mean()

    # 3. é¢„æµ‹ä¸€è‡´æ€§ (ä¸¤ä¸ªæ¨¡å‹é¢„æµ‹ç»“æœç›¸åŒçš„æ¯”ä¾‹)
    consistency = np.mean(tflite_preds == keras_preds)
    
    print("\n--- ç²¾åº¦éªŒè¯æŠ¥å‘Š ---")
    print(f"æ ·æœ¬æ€»æ•°: {len(test_data)}")
    print(f"1. é¢„æµ‹ä¸€è‡´æ€§ (INT8 vs Float32): {consistency:.4f} ({consistency*100:.2f}%)")
    print(f"2. å¹³å‡è¾“å‡º L2 å·®å¼‚ (è¶Šå°è¶Šå¥½): {output_difference:.6f}")
    
    if np.any(true_labels != 0): # åªæœ‰å½“æ ‡ç­¾æ˜¯çœŸå®å€¼æ—¶æ‰è®¡ç®—ç²¾åº¦
        print(f"3. Keras (Float32) Top-1 ç²¾åº¦: {keras_accuracy:.4f}")
        print(f"4. TFLite (INT8) Top-1 ç²¾åº¦: {tflite_accuracy:.4f}")
        print(f"é‡åŒ–æŸå¤± (Float32 - INT8): {(keras_accuracy - tflite_accuracy)*100:.2f}%")
    else:
        print("æ³¨æ„: ç”±äºç¼ºä¹çœŸå®æ ‡ç­¾ï¼Œæ— æ³•è®¡ç®— Top-1 ç²¾åº¦ã€‚")

# =================================================================
# 5. ä¸»æ‰§è¡Œé€»è¾‘
# =================================================================

if __name__ == '__main__':
    # 1. åŠ è½½æµ‹è¯•æ•°æ®
    test_data = load_test_data(NUM_TEST_SAMPLES)
    if not test_data:
        print("é”™è¯¯: æœªåŠ è½½åˆ°ä»»ä½•æµ‹è¯•æ•°æ®ã€‚è¯·æ£€æŸ¥ IMAGENET_VAL_DIR è·¯å¾„ã€‚")
    else:
        # 2. Keras æ¨ç† (Float32 åŸºå‡†)
        keras_outputs = run_keras_inference(keras_model, test_data)
        
        # 3. TFLite Interpreter æ¨ç† (é‡åŒ–æ¨¡å‹)
        tflite_outputs = run_tflite_inference(TFLITE_MODEL_PATH, test_data)
        
        # 4. æŠ¥å‘Šç»“æœ
        evaluate_and_report(tflite_outputs, keras_outputs, test_data)
        
    print("\nâœ… é‡åŒ–æ¨¡å‹éªŒè¯æµç¨‹ç»“æŸã€‚")